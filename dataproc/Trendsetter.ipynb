{"cells":[{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["########################\n","###### Changerate ######\n","########################\n","\n","# Imports\n","import pymongo, urllib, json, datetime, psycopg2\n","from pymongo import MongoClient\n","from pyspark.sql.types import StructType, StructField, IntegerType, StringType\n","from functools import reduce\n","from pyspark import SparkContext\n","from pyspark.sql import SparkSession\n","\n","# Connect to MongoDB\n","pw = \"\"\n","cluster = MongoClient('mongodb+srv:')\n","db = cluster[\"trends\"]\n","collection_berlin = db[\"berlin\"]\n","\n","# Get trends from MongoDB\n","trends_berlin = collection_berlin.find()\n","\n","# Close connection to MongoDB\n","cluster.close()\n","\n","# If not already created automatically, instantiate Sparkcontext\n","sc = SparkContext.getOrCreate()\n","spark = SparkSession.builder.getOrCreate()\n","\n","# Connect to Postgres\n","conn = psycopg2.connect(host=\"\", port = 5432, database=\"trends\", user=\"\", password=\"\")\n","\n","# Get or create Spark Context and Spark Session\n","sc = SparkContext.getOrCreate()\n","spark = SparkSession.builder.getOrCreate()\n","\n","# Create empty list for dataframes\n","df_list_trends_berlin = list()\n","\n","# Iterate over list of trends\n","counter_trends = 1\n","for trends in trends_berlin:\n","    \n","    # Create PySpark dataframe\n","    df_trends = spark.createDataFrame([], StringType())\n","\n","    # Iterate over trends at listelement\n","    counter = 0\n","    while counter <= 49:\n","        \n","        # Append to list of dataframes\n","        newRow = spark.createDataFrame([trends[\"trends\"][counter][\"name\"].replace(\"#\",\"\")], StringType())\n","        df_trends = df_trends.union(newRow)\n","        counter+=1\n","        \n","    # Append to list of dataframes\n","    df_list_trends_berlin.append(df_trends)\n","    \n","    # Every second\n","    if counter_trends == 2:\n","        \n","        # Join dataframes of trends by equal values\n","        df_equality = df_list_trends_berlin[0].join(df_list_trends_berlin[1], [df_list_trends_berlin[0].value == df_list_trends_berlin[1].value] , how = 'inner' )\n","        \n","        # Calculate changerate by subtract equality from 100%\n","        equality = df_equality.count()/50\n","        changerate = round(1 - equality, 2)\n","        changerate = changerate * 100\n","        \n","        # Get datetime of trends\n","        date = trends[\"as_of\"].replace(\"T\",\" \").replace(\"Z\",\"\")\n","        date = datetime.datetime.strptime(date, '%Y-%m-%d %H:%M:%S')\n","        date = date + datetime.timedelta(hours=2)\n","\n","        # Insert data into PostgreSQL\n","        cur = conn.cursor()\n","        cur.execute(\"\"\"INSERT INTO Changerate (changerate, date) VALUES (\"\"\"+str(changerate)+\"\"\",'\"\"\"+str(date)+\"\"\"')\"\"\")\n","        conn.commit()\n","        cur.close()\n","        \n","        # Delete first element\n","        del df_list_trends_berlin[0]\n","        counter_trends = 1\n","        \n","        # Print results\n","        print(\"Changerate: \"+str(changerate)+\"\")\n","        print(\"Datetime: \"+str(date)+\"\")\n","        print(\"\"\"INSERT INTO Changerate (changerate, date) VALUES (\"\"\"+str(changerate)+\"\"\",'\"\"\"+str(date)+\"\"\"')\"\"\")\n","        print(\"----------------------------------------------------------------------------------\")\n","        \n","    counter_trends+=1\n","\n","# Close connection to DB and Spark\n","conn.close()\n","spark.stop()"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]}],"metadata":{"kernelspec":{"display_name":"PySpark","language":"python","name":"pyspark"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.4"}},"nbformat":4,"nbformat_minor":4}