{"cells":[{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["########################\n","#### Trend equality ####\n","########################\n","\n","# Imports\n","import pymongo, urllib, json, datetime, psycopg2\n","from pymongo import MongoClient\n","from pyspark.sql.types import StructType, StructField, IntegerType, StringType\n","from functools import reduce\n","from pyspark import SparkContext\n","from pyspark.sql import SparkSession\n","\n","# Connect to MongoDB\n","pw = \"\"\n","cluster = MongoClient('mongodb+srv:')\n","db = cluster[\"trends\"]\n","collection_berlin = db[\"berlin\"]\n","collection_stuttgart = db[\"stuttgart\"]\n","\n","# Get trends from MongoDB\n","trends_berlin = collection_berlin.find()\n","trends_stuttgart = collection_stuttgart.find()\n","\n","# Close MongoDB connection\n","cluster.close()\n","\n","# Connect to Postgres\n","conn = psycopg2.connect(host=\"\", port = 5432, database=\"trends\", user=\"\", password=\"\")\n","\n","# Get or create Spark Context and Spark Session\n","sc = SparkContext.getOrCreate()\n","spark = SparkSession.builder.getOrCreate()\n","\n","# Create empty list for dataframes\n","df_list_trends = list()\n","\n","# Iterate over list of trends\n","for trends_b, trends_s in zip(trends_berlin, trends_stuttgart):\n","    \n","    # Create PySpark dataframe\n","    df_trends_b = spark.createDataFrame([], StringType())\n","    df_trends_s = spark.createDataFrame([], StringType())\n","    \n","    # Iterate over trends at listelement\n","    counter = 0\n","    while counter <= 49:\n","        \n","        # Create dataframes with trends\n","        newRow_b = spark.createDataFrame([trends_b[\"trends\"][counter][\"name\"].replace(\"#\",\"\")], StringType())\n","        newRow_s = spark.createDataFrame([trends_s[\"trends\"][counter][\"name\"].replace(\"#\",\"\")], StringType())\n","        df_trends_b = df_trends_b.union(newRow_b)\n","        df_trends_s = df_trends_s.union(newRow_s)\n","        counter+=1\n","    \n","    # Append to list of dataframes\n","    df_list_trends.append(df_trends_b)\n","    df_list_trends.append(df_trends_s)\n","    \n","    # Join dataframes of trends by equal values\n","    df_trend_equality = df_list_trends[0].join(df_list_trends[1], [df_list_trends[0].value == df_list_trends[1].value] , how = 'inner' )\n","    \n","    # Calculate trend equality by counting equal values\n","    trend_equality = df_trend_equality.count()/50\n","    trend_equality = round((trend_equality * 100), 0)\n","    \n","    # Get datetime of trends\n","    date = trends_b[\"as_of\"].replace(\"T\",\" \").replace(\"Z\",\"\")\n","    date = datetime.datetime.strptime(date, '%Y-%m-%d %H:%M:%S')\n","    date = date + datetime.timedelta(hours=2)\n","    \n","    # Insert data into PostgreSQL\n","    cur = conn.cursor()\n","    cur.execute(\"\"\"INSERT INTO Trend_equality (trend_equality, date) VALUES (\"\"\"+str(trend_equality)+\"\"\",'\"\"\"+str(date)+\"\"\"')\"\"\")\n","    conn.commit()\n","    cur.close()\n","    \n","    # Clear list of dataframes\n","    df_list_trends = list()\n","    \n","    # Print results\n","    print(\"-----------------------------------------------------------------------------\")\n","    print(\"Trend equality: \"+str(trend_equality)+\"\")\n","    print(\"Datetime: \"+str(date)+\"\")\n","    print(\"\"\"INSERT INTO Trend_equality (trend_equality, date) VALUES (\"\"\"+str(trend_equality)+\"\"\",'\"\"\"+str(date)+\"\"\"')\"\"\")\n","    print(\"-----------------------------------------------------------------------------\")\n","\n","# Close connection to DB and Spark\n","conn.close()\n","spark.stop()"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]}],"metadata":{"kernelspec":{"display_name":"PySpark","language":"python","name":"pyspark"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.4"}},"nbformat":4,"nbformat_minor":4}